{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04c2b38-dd8f-4b96-967e-6ef2b238e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cc5a45-ecba-407b-a8e2-89c5c1c22314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import scipy.io\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.color import rgb2gray\n",
    "from scipy.ndimage import convolve, gaussian_filter\n",
    "from numpy.fft import fft2, ifft2, fftshift\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.signal import convolve2d\n",
    "from PIL import Image\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.ndimage import gaussian_filter, maximum_filter\n",
    "from scipy.sparse import spdiags, eye\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.signal import fftconvolve\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab822f1f-157d-45e6-bde8-49fe2b2a6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_rgb(image, kernel):\n",
    "    border_size = 1\n",
    "    foggy_image_with_border = cv2.copyMakeBorder(image,\n",
    "                                                 top=border_size,\n",
    "                                                 bottom=border_size,\n",
    "                                                 left=border_size,\n",
    "                                                 right=border_size,\n",
    "                                                 borderType=cv2.BORDER_WRAP)\n",
    "\n",
    "    # Apply the filter\n",
    "    temp = cv2.filter2D(foggy_image_with_border, -1, kernel)\n",
    "\n",
    "    # Remove the added border\n",
    "    temp = temp[border_size:-border_size, border_size:-border_size]\n",
    "\n",
    "    return temp\n",
    "\n",
    "def rgb2gray(rgb_image):\n",
    "    \"\"\"\n",
    "    Convert an RGB image to a grayscale image.\n",
    "    :param rgb_image: Input RGB image as a NumPy array.\n",
    "    :return: Grayscale image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Check if the input image is a 3D array (RGB image)\n",
    "    if rgb_image.ndim != 3 or rgb_image.shape[2] != 3:\n",
    "        raise ValueError(\"Input must be an RGB image\")\n",
    "\n",
    "    # Define the coefficients for the RGB channels\n",
    "    coeffs = np.array([0.2989, 0.5870, 0.1140])\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = np.dot(rgb_image[..., :3], coeffs)\n",
    "\n",
    "    return gray_image\n",
    "\n",
    "def convolve_rgb(image, kernel):\n",
    "    border_size = 1\n",
    "    foggy_image_with_border = cv2.copyMakeBorder(image,\n",
    "                                                 top=border_size,\n",
    "                                                 bottom=border_size,\n",
    "                                                 left=border_size,\n",
    "                                                 right=border_size,\n",
    "                                                 borderType=cv2.BORDER_WRAP)\n",
    "\n",
    "    # Apply the filter\n",
    "    temp = cv2.filter2D(foggy_image_with_border, -1, kernel)\n",
    "\n",
    "    # Remove the added border\n",
    "    temp = temp[border_size:-border_size, border_size:-border_size]\n",
    "\n",
    "    return temp\n",
    "\n",
    "def gradient_weight(I):\n",
    "    if len(I.shape) == 3 and I.shape[2] == 3:  # If the image has RGB channels\n",
    "        I = np.dot(I[..., :3], [0.2989, 0.5870, 0.1140])  # Convert to grayscale using standard weights\n",
    "\n",
    "    lambda_val = 10\n",
    "    f1 = np.array([[0, 0, 0], [1, -1, 0], [0, 0, 0]])\n",
    "    f2 = np.array([[0, 1, 0], [0, -1, 0], [0, 0, 0]])\n",
    "\n",
    "    Gx = signal.convolve2d(I, f1, mode='same')\n",
    "    Gy = signal.convolve2d(I, f2, mode='same')\n",
    "\n",
    "    ax = np.exp(-lambda_val * np.abs(Gx))\n",
    "    thx = Gx < 0.01\n",
    "    ax[thx] = 0\n",
    "    weight_x = 1 + ax\n",
    "\n",
    "    ay = np.exp(-lambda_val * np.abs(Gy))\n",
    "    thy = Gy < 0.01\n",
    "    ay[thy] = 0\n",
    "    weight_y = 1 + ay\n",
    "\n",
    "    return weight_x, weight_y\n",
    "\n",
    "\n",
    "def psf2otf(psf, shape):\n",
    "    psf = np.pad(psf, [(0, shape[0] - psf.shape[0]), (0, shape[1] - psf.shape[1])], mode='constant')\n",
    "    for axis, axis_size in enumerate(psf.shape):\n",
    "        psf = np.roll(psf, -axis_size // 2, axis=axis)\n",
    "    otf = fft2(psf.T).T\n",
    "    return otf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51acc5ed-fefa-4802-915d-e062eacb4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_training(input_img):\n",
    "    # foggy_image_path = './Test_Data/V_08_01_0038.jpg' # This is relative path which needs to be updated.\n",
    "    # mat_contents = scipy.io.loadmat('./mat_data_file/V_08_01_0038.mat')\n",
    "    # ii = mat_contents['ii']\n",
    "    # A = mat_contents['A']\n",
    "    ii = 5\n",
    "\n",
    "    # Read the image using OpenCV\n",
    "    adjust_fog_removal = 2\n",
    "    brightness = 2\n",
    "\n",
    "    # Convert image from uint8 to float32 (similar to im2double)\n",
    "\n",
    "    I = input_img.copy()\n",
    "    alpha = 20000\n",
    "    beta = 0.1\n",
    "    gamma = 10\n",
    "\n",
    "    alpha = float(alpha)\n",
    "    ii = float(ii)\n",
    "    beta = float(beta)\n",
    "    gamma = float(gamma)\n",
    "    I = np.clip(I, 0, 1)\n",
    "    gray = rgb2gray(I)\n",
    "    H, W, D = I.shape\n",
    "\n",
    "    weight_x, weight_y = gradient_weight(I)\n",
    "    # f1 = np.array([[1, -1]])\n",
    "    # f2 = np.array([[1], [-1]])\n",
    "    f1 = np.array([[0, 0, 0], [1, -1, 0], [0, 0, 0]])\n",
    "    f2 = np.array([[0, 1, 0], [0, -1, 0], [0, 0, 0]])\n",
    "    f4 = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n",
    "\n",
    "    gray = rgb2gray(I)\n",
    "    I_filt = gaussian_filter(gray, sigma=10)\n",
    "    delta_I = I - I_filt[..., np.newaxis]\n",
    "\n",
    "    otfFx = psf2otf(f1, (H, W))\n",
    "    otfFy = psf2otf(f2, (H, W))\n",
    "    otfL = psf2otf(f4, (H, W))\n",
    "\n",
    "    fft_double_laplace = np.abs(otfL) ** 2\n",
    "    fft_double_grad = np.abs(otfFx) ** 2 + np.abs(otfFy) ** 2\n",
    "\n",
    "    if D > 1:\n",
    "        fft_double_grad = np.repeat(fft_double_grad[:, :, np.newaxis], D, axis=2)\n",
    "        fft_double_laplace = np.repeat(fft_double_laplace[:, :, np.newaxis], D, axis=2)\n",
    "        weight_x = np.repeat(weight_x[:, :, np.newaxis], D, axis=2)\n",
    "        weight_y = np.repeat(weight_y[:, :, np.newaxis], D, axis=2)\n",
    "\n",
    "    F = np.zeros_like(I)\n",
    "    N = np.zeros_like(I)\n",
    "    gray = rgb2gray(I)\n",
    "    Ix = np.zeros_like(I)\n",
    "    Iy = np.zeros_like(I)\n",
    "\n",
    "    for channel in range(I.shape[2]):\n",
    "        Ix[:, :, channel] = signal.convolve2d(I[:, :, channel], f1, mode='same')\n",
    "        Iy[:, :, channel] = signal.convolve2d(I[:, :, channel], f2, mode='same')\n",
    "\n",
    "    Normin_I = fft2((np.concatenate((Ix[:, -1:, :] - Ix[:, :1, :], -np.diff(Ix, 1, 1)), axis=1) +\n",
    "                     np.concatenate((Iy[-1:, :, :] - Iy[:1, :, :], -np.diff(Iy, 1, 0)), axis=0)).T).T\n",
    "    Denormin_N = gamma + alpha * fft_double_laplace + beta\n",
    "    Normin_gI = fft_double_laplace * fft2(I.T).T\n",
    "\n",
    "    return(F, I, N, Normin_I, Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, weight_x, weight_y, delta_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52abba5-b82d-4c41-8c67-4035802f6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a3fde3-a8a0-4c48-9e66-7882e3b74bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = glob('./Test_Data/*')\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c7fc6a-e111-4619-b388-e7c87161ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F_list = []\n",
    "# I_list = []\n",
    "# N_list = []\n",
    "# Normin_I_list = []\n",
    "# Normin_gI_list = []\n",
    "# Denormin_N_list = []\n",
    "# Ix_list = []\n",
    "# Iy_list = []\n",
    "# fft_double_grad_list = []\n",
    "# fft_double_laplace_list = []\n",
    "# weight_x_list = []\n",
    "# weight_y_list = []\n",
    "# delta_I_list = []\n",
    "\n",
    "# for i in tqdm(range(len(paths))):\n",
    "#     foggy_image_path = paths[i]\n",
    "#     foggy_image = cv2.imread(foggy_image_path)\n",
    "#     foggy_image = cv2.cvtColor(foggy_image, cv2.COLOR_BGR2RGB)\n",
    "#     input_img = foggy_image.astype(np.float32) / foggy_image.max()\n",
    "#     F = preprocess_for_training(input_img)\n",
    "#     inputs = F.copy()\n",
    "#     inputs = np.array([np.moveaxis(inputs,-1,0)])\n",
    "#     inputs = torch.from_numpy(inputs).float()\n",
    "#     inputs = inputs\n",
    "#     outputs = model(inputs)\n",
    "#     out = np.moveaxis(outputs.cpu().detach().numpy()[0,:,:,:],0,-1)\n",
    "#     out = out-np.min(out)\n",
    "#     out = out/np.max(out)\n",
    "#     out2 = preprocess_for_training(out)\n",
    "#     input_images.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8ff4f-6a54-4afc-ba2b-f22bb38c8ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b232141a-b1db-40bd-b7d6-dffb6f2a9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea68968-2cd1-4ae4-8d51-1a2ae1a6dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batchnorm_layer(opts):\n",
    "    if opts.norm_layer == \"batch\":\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    elif opts.layer == \"spectral_instance\":\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "    else:\n",
    "        print(\"not implemented\")\n",
    "        exit()\n",
    "    return norm_layer\n",
    "\n",
    "def get_conv2d_layer(in_c, out_c, k, s, p=0, dilation=1, groups=1):\n",
    "    return nn.Conv2d(in_channels=in_c,\n",
    "                    out_channels=out_c,\n",
    "                    kernel_size=k,\n",
    "                    stride=s,\n",
    "                    padding=p,dilation=dilation, groups=groups)\n",
    "\n",
    "def get_deconv2d_layer(in_c, out_c, k=1, s=1, p=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
    "        nn.Conv2d(\n",
    "            in_channels=in_c,\n",
    "            out_channels=out_c,\n",
    "            kernel_size=k,\n",
    "            stride=s,\n",
    "            padding=p\n",
    "        )\n",
    "    )\n",
    "\n",
    "class Identity(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b367bd-61ba-40cf-80da-83394cb3ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decom = nn.Sequential(\n",
    "            get_conv2d_layer(in_c=3, out_c=32, k=3, s=1, p=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            get_conv2d_layer(in_c=32, out_c=32, k=3, s=1, p=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            get_conv2d_layer(in_c=32, out_c=32, k=3, s=1, p=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            get_conv2d_layer(in_c=32, out_c=4, k=3, s=1, p=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.decom(input)\n",
    "        R = output[:, 0:3, :, :]\n",
    "        L = output[:, 3:4, :, :]\n",
    "        return R, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eccb25a8-cc85-43b4-9472-4645e582688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.modules.linear import Identity\n",
    "import math\n",
    "import torch.nn.functional as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe576ce-313c-44ed-8161-b9e34f555f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfDnCNNSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = get_conv2d_layer(in_c=3, out_c=32, k=3, s=1, p=1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = get_conv2d_layer(in_c=1, out_c=32, k=3, s=1, p=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.se_layer = SELayer(channel=64)\n",
    "        self.conv3 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.conv7 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv8 = get_conv2d_layer(in_c=64, out_c=3, k=3, s=1, p=1)\n",
    "\n",
    "    def forward(self, r, l):\n",
    "        r_fs = self.relu1(self.conv1(r))\n",
    "        l_fs = self.relu2(self.conv2(l))\n",
    "        inf = torch.cat([r_fs, l_fs], dim=1)\n",
    "        se_inf = self.se_layer(inf)\n",
    "        x1 = self.relu3(self.conv3(se_inf))\n",
    "        x2 = self.relu4(self.conv4(x1))\n",
    "        x3 = self.relu5(self.conv5(x2))\n",
    "        x4 = self.relu6(self.conv6(x3))\n",
    "        x5 = self.relu7(self.conv7(x4))\n",
    "        n = self.conv8(x5)\n",
    "        r_restore = r + n\n",
    "        return r_restore\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bbdc48d-12f1-4eda-97ba-26a6e8bffa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model1 = Decom()\n",
    "        self.model2 = HalfDnCNNSE()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1,x2 = self.model1(x)\n",
    "        x3 = self.model2(x1,x2)\n",
    "        return x3\n",
    "\n",
    "\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "net3 = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611be80-d341-4b09-9a1d-b4171759d5d7",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8f47a005-43bf-45e0-95a2-1d39a9568428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L_color(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_color, self).__init__()\n",
    "\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Drg = torch.pow(mr-mg,2)\n",
    "        Drb = torch.pow(mr-mb,2)\n",
    "        Dgb = torch.pow(mb-mg,2)\n",
    "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
    "\n",
    "\n",
    "        return k\n",
    "\n",
    "\t\t\t\n",
    "class L_spa(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_spa, self).__init__()\n",
    "        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).unsqueeze(0).unsqueeze(0)\n",
    "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
    "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
    "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
    "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
    "        self.pool = nn.AvgPool2d(4)\n",
    "    def forward(self, org , enhance ):\n",
    "        b,c,h,w = org.shape\n",
    "\n",
    "        org_mean = torch.mean(org,1,keepdim=True)\n",
    "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
    "\n",
    "        org_pool =  self.pool(org_mean)\t\t\t\n",
    "        enhance_pool = self.pool(enhance_mean)\t\n",
    "\n",
    "        weight_diff =torch.max(torch.FloatTensor([1]) + 10000*torch.min(org_pool - torch.FloatTensor([0.3]),torch.FloatTensor([0])),torch.FloatTensor([0.5]))\n",
    "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5])) ,enhance_pool-org_pool)\n",
    "\n",
    "\n",
    "        D_org_letf = Func.conv2d(org_pool , self.weight_left, padding=1)\n",
    "        D_org_right = Func.conv2d(org_pool , self.weight_right, padding=1)\n",
    "        D_org_up = Func.conv2d(org_pool , self.weight_up, padding=1)\n",
    "        D_org_down = Func.conv2d(org_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_enhance_letf = Func.conv2d(enhance_pool , self.weight_left, padding=1)\n",
    "        D_enhance_right = Func.conv2d(enhance_pool , self.weight_right, padding=1)\n",
    "        D_enhance_up = Func.conv2d(enhance_pool , self.weight_up, padding=1)\n",
    "        D_enhance_down = Func.conv2d(enhance_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
    "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
    "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
    "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
    "        E = (D_left + D_right + D_up +D_down)\n",
    "        # E = 25*(D_left + D_right + D_up +D_down)\n",
    "\n",
    "        return E\n",
    "class L_exp(nn.Module):\n",
    "\n",
    "    def __init__(self,patch_size,mean_val):\n",
    "        super(L_exp, self).__init__()\n",
    "        # print(1)\n",
    "        self.pool = nn.AvgPool2d(patch_size)\n",
    "        self.mean_val = mean_val\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "        x = torch.mean(x,1,keepdim=True)\n",
    "        mean = self.pool(x)\n",
    "\n",
    "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ),2))\n",
    "        return d\n",
    "        \n",
    "class L_TV(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=1):\n",
    "        super(L_TV,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
    "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "class Sa_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sa_Loss, self).__init__()\n",
    "        # print(1)\n",
    "    def forward(self, x ):\n",
    "        # self.grad = np.ones(x.shape,dtype=np.float32)\n",
    "        b,c,h,w = x.shape\n",
    "        # x_de = x.cpu().detach().numpy()\n",
    "        r,g,b = torch.split(x , 1, dim=1)\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Dr = r-mr\n",
    "        Dg = g-mg\n",
    "        Db = b-mb\n",
    "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
    "        # print(k)\n",
    "        \n",
    "\n",
    "        k = torch.mean(k)\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26d65a25-8e43-4179-8601-698a33d46ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_color = L_color()\n",
    "L_spa = L_spa()\n",
    "\n",
    "L_exp = L_exp(16,0.6)\n",
    "L_TV = L_TV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f7b08-248d-402a-a9af-78a0fb8644a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "db7511aa-1900-4d43-8d3f-7170291a2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fft\n",
    "import torch.nn.functional as Ff\n",
    "\n",
    "def torch_convolve_rgb(image, kernel):\n",
    "    # PyTorch's conv2d expects a 4D tensor of shape (batch_size, channels, height, width)\n",
    "    # Adjust the input image tensor to have channels as the second dimension\n",
    "    if image.dim() == 3:  # If the image is of shape (H, W, C)\n",
    "        image = image.permute(2, 0, 1)  # Change to (C, H, W)\n",
    "    image = image.unsqueeze(0)  # Add the batch dimension\n",
    "\n",
    "    # Adjusting kernel shape for PyTorch (out_channels, in_channels, height, width)\n",
    "    # Assuming kernel is a square matrix\n",
    "    kernel_size = kernel.shape[0]\n",
    "    kernel = kernel.expand(3, 1, kernel_size, kernel_size)\n",
    "\n",
    "    # Convolution\n",
    "    convolved = Ff.conv2d(image, kernel, padding=1, groups=3)\n",
    "    convolved = convolved.squeeze(0)  # Remove the batch dimension\n",
    "\n",
    "    # Rearrange the tensor back to (H, W, C) if necessary\n",
    "    if convolved.dim() == 3:\n",
    "        convolved = convolved.permute(1, 2, 0)\n",
    "\n",
    "    return convolved\n",
    "    \n",
    "\n",
    "def create_gaussian_kernel(sigma, truncate):\n",
    "    radius = int(truncate * sigma + 0.5)\n",
    "    size = 2 * radius + 1\n",
    "    x = torch.arange(-radius, radius + 1, dtype=torch.float32)\n",
    "    kernel = torch.exp(-0.5 / (sigma ** 2) * x ** 2)\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "def gaussian_filter_pytorch(input, sigma, truncate=4.0):\n",
    "    kernel = create_gaussian_kernel(sigma, truncate)\n",
    "    kernel_size = kernel.shape[0]\n",
    "    kernel = kernel.view(1, 1, *kernel.shape)\n",
    "\n",
    "    input_channels = input.size(1)\n",
    "    kernel = kernel.repeat(input_channels, 1, 1, 1)\n",
    "\n",
    "    padding = kernel_size // 2\n",
    "    filtered = Ff.conv2d(input, kernel, padding=padding, groups=input_channels)\n",
    "\n",
    "    # Calculate cropping to match input dimensions\n",
    "    crop_h = (filtered.size(2) - input.size(2)) // 2\n",
    "    crop_w = (filtered.size(3) - input.size(3)) // 2\n",
    "    filtered_cropped = filtered[:, :, crop_h:filtered.size(2)-crop_h, crop_w:filtered.size(3)-crop_w]\n",
    "\n",
    "    return filtered_cropped\n",
    "\n",
    "def compute_decomposition_step_dev(stage,F, I, N, Normin_I,  Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, alpha, beta, gamma, weight_x, weight_y, delta_I, H, W, Dd, i, ii):\n",
    "    if(stage==0):\n",
    "        F = torch.from_numpy(F).float()\n",
    "        N = torch.from_numpy(N).float()        \n",
    "    I = torch.from_numpy(I).float()\n",
    "    Normin_I = torch.from_numpy(Normin_I).cfloat()\n",
    "    Normin_gI = torch.from_numpy(Normin_gI).cfloat()\n",
    "    Ix = torch.from_numpy(Ix).float()\n",
    "    Iy = torch.from_numpy(Iy).float()\n",
    "    fft_double_grad = torch.from_numpy(fft_double_grad).float()\n",
    "    fft_double_laplace = torch.from_numpy(fft_double_laplace).float()\n",
    "    weight_x = torch.from_numpy(weight_x).float()\n",
    "    weight_y = torch.from_numpy(weight_y).float()\n",
    "    delta_I = torch.from_numpy(delta_I).float()\n",
    "    Denormin_N = torch.from_numpy(Denormin_N)\n",
    "\n",
    "    \n",
    "    lambda_ = min(2 ** (ii + i), 10 ** 5)\n",
    "    Denormin_F = lambda_ * fft_double_grad + alpha * fft_double_laplace + beta\n",
    "       \n",
    "    f1_tensor = torch.tensor([[0, 0, 0], [1, -1, 0], [0, 0, 0]], dtype=torch.float32)\n",
    "    f2_tensor = torch.tensor([[0, 1, 0], [0, -1, 0], [0, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # Update q using the tensor versions of the filters\n",
    "    qx = -torch_convolve_rgb(F, f1_tensor) - Ix\n",
    "    qy = -torch_convolve_rgb(F, f2_tensor) - Iy\n",
    "        \n",
    "    qx = torch.sign(qx) * torch.clamp(torch.abs(qx) - weight_x / lambda_, min=0)\n",
    "    qy = torch.sign(qy) * torch.clamp(torch.abs(qy) - weight_y / lambda_, min=0)\n",
    "\n",
    "\n",
    "    Normin_q_x = torch.cat((qx[:, -1:, :] - qx[:,:1, :], -torch.diff(qx, dim=1)), dim=1)\n",
    "    Normin_q_y = torch.cat((qy[-1:, :, :] - qy[:1, :], -torch.diff(qy, dim=0)), dim=0)\n",
    "    Normin_q = Normin_q_x + Normin_q_y\n",
    "\n",
    "    N_expanded = N.unsqueeze(2) if N.dim() == 2 else N\n",
    "    # print(\"N_expanded dimensions:\", N_expanded.shape)\n",
    "    Normin_gN = fft_double_laplace * torch.fft.fft2(N_expanded.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "    FF = (lambda_ * (Normin_I + torch.fft.fft2(Normin_q.permute(2, 0, 1)).permute(1, 2, 0)) + alpha * (Normin_gI - Normin_gN) + beta * torch.fft.fft2((delta_I - N_expanded).permute(2, 0, 1)).permute(1, 2, 0)) / Denormin_F\n",
    "    F = torch.fft.ifft2(FF.permute(2, 0, 1)).permute(1, 2, 0).real\n",
    "\n",
    "\n",
    "    Normin_F = fft_double_laplace * torch.fft.fft2(F.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "    B = torch.fft.fft2((delta_I - F).permute(2, 0, 1)).permute(1, 2, 0)\n",
    "    NN = (alpha * (Normin_gI - Normin_F) + beta * B) / Denormin_N\n",
    "    # NN = torch.from_numpy(NN).float()\n",
    "    N = torch.fft.ifft2(NN.permute(2, 0, 1)).permute(1, 2, 0).real\n",
    "    N = N.squeeze() if N.dim() == 2 else N\n",
    "\n",
    "    for c in range(Dd):\n",
    "        Ft = F[:, :, c]\n",
    "        q = torch.numel(Ft)\n",
    "        for k in range(500):\n",
    "            m = torch.sum(Ft[Ft < 0])\n",
    "            n = torch.sum(Ft[Ft > 1] - 1)\n",
    "            dt = (m + n) / q\n",
    "            if torch.abs(dt) < 1 / q:\n",
    "                break\n",
    "            Ft = Ft - dt\n",
    "        F[:, :, c] = Ft\n",
    "\n",
    "    F = torch.abs(F)\n",
    "    F = torch.clamp(F, max=1)\n",
    "    N = torch.clamp(N, max=1, min=0)\n",
    "    N = torch.mean(N, dim=2)\n",
    "    \n",
    "    # # Compute G\n",
    "    G = torch.abs(I - F - N.unsqueeze(2))\n",
    "    G = torch.min(G, dim=2).values\n",
    "    \n",
    "    # # Apply Gaussian filter (assuming gaussian_kernel function is defined and returns a PyTorch tensor)\n",
    "    G = G.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions if needed\n",
    "    G = G.float()\n",
    "    G = gaussian_filter_pytorch(G, sigma=3)\n",
    "    G = G.squeeze(0).squeeze(0)\n",
    "\n",
    "    \n",
    "    G_expanded = G.unsqueeze(-1)  # Shape becomes (358, 640, 1)\n",
    "    N_expanded = N.unsqueeze(-1)  # Shape becomes (358, 640, 1)\n",
    "    \n",
    "    # Perform the operation\n",
    "    F = torch.abs(I - G_expanded - N_expanded)\n",
    "    \n",
    "    # Replace zeros with 0.001\n",
    "    F[F == 0] = 0.001\n",
    "    \n",
    "    # delta_I = delta_I.numpy()\n",
    "    # Normin_I = Normin_I.numpy()\n",
    "    # Normin_q = Normin_q.numpy()\n",
    "    # weight_x = weight_x.numpy()\n",
    "    # weight_y = weight_y.numpy()\n",
    "    # # F = F.numpy()\n",
    "    # Ix = Ix.numpy()\n",
    "    # Iy = Iy.numpy()\n",
    "    # qx = qx.numpy()\n",
    "    # qy = qy.numpy()\n",
    "    # fft_double_grad = fft_double_grad.numpy()\n",
    "    # fft_double_laplace = fft_double_laplace.numpy()\n",
    "    # Denormin_F = Denormin_F.numpy()\n",
    "    # # N = N.numpy()\n",
    "    # N_expanded = N_expanded.numpy()\n",
    "    # Normin_gN = Normin_gN.numpy()\n",
    "    # Normin_gI = Normin_gI.numpy()\n",
    "    # I = I.numpy()\n",
    "    # G = G.numpy() \n",
    "\n",
    "    return F, G, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09d2aa60-4a2b-400a-9e12-ece689a347ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 1000, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694fe51-08ce-42a8-adf7-6296aa2fcbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f755151e-382a-4cc3-823d-b374fbec3247",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dfb44c6-0010-4a5a-ba61-81409551bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(list(net1.parameters())+list(net2.parameters())+list(net3.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede48cc-4d63-4d7a-9fdc-1c6ab29ed8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1a6f6-bb30-4ab1-8d77-2603b5f5cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100cd4c4-fc4c-4204-a1ae-c32a396d9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14623561-c566-474b-8694-bd09e1667721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "step :  8\n",
      "loss :  tensor(3.2215, grad_fn=<AddBackward0>)  to  tensor(1.2272, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "step = 0\n",
    "losses = []\n",
    "for i in range(20):\n",
    "    for j in range(len(paths)-10):\n",
    "        clear_output(wait=True)\n",
    "        print('epoch : ',epoch)\n",
    "        print('step : ',step)\n",
    "        if(len(losses)!=0):\n",
    "            print('loss : ',losses[0],' to ',losses[-1]) \n",
    "        step = step+1\n",
    "        optimizer.zero_grad()\n",
    "        foggy_image_path = paths[j]\n",
    "        foggy_image = cv2.imread(foggy_image_path)\n",
    "        foggy_image = cv2.cvtColor(foggy_image, cv2.COLOR_BGR2RGB)\n",
    "        input_img = foggy_image.astype(np.float32) / foggy_image.max()\n",
    "        H, W, D = input_img.shape\n",
    "        alpha = torch.tensor([20000])\n",
    "        ii = torch.tensor([5])\n",
    "        beta = torch.tensor([0.1])\n",
    "        gamma = torch.tensor([10])\n",
    "        F, I, N, Normin_I, Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, weight_x, weight_y, delta_I = preprocess_for_training(input_img)\n",
    "        F, G, N = compute_decomposition_step_dev(0,F, I, N, Normin_I, Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, \n",
    "                                             alpha, beta, gamma, weight_x, weight_y, delta_I, H, W, D, 0, ii)\n",
    "\n",
    "        F = torch.unsqueeze(torch.moveaxis(F, -1, 0),0).float()\n",
    "        F = net1(F)\n",
    "        F = torch.moveaxis(F[0,:,:,:], 0, -1)\n",
    "        F = F.float()\n",
    "        N = N.float()\n",
    "        F, G, N = compute_decomposition_step_dev(1,F, I, N, Normin_I, Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, \n",
    "                                             alpha, beta, gamma, weight_x, weight_y, delta_I, H, W, D, 1, ii)\n",
    "        F = torch.unsqueeze(torch.moveaxis(F, -1, 0),0).float()\n",
    "        F = net2(F)\n",
    "        F = torch.moveaxis(F[0,:,:,:], 0, -1)\n",
    "        F = F.float()\n",
    "        N = N.float()\n",
    "        F, G, N = compute_decomposition_step_dev(2,F, I, N, Normin_I, Normin_gI, Denormin_N, Ix, Iy, fft_double_grad, fft_double_laplace, \n",
    "                                             alpha, beta, gamma, weight_x, weight_y, delta_I, H, W, D, 2, ii)\n",
    "        F = torch.unsqueeze(torch.moveaxis(F, -1, 0),0).float()\n",
    "        F1 = net3(F)\n",
    "\n",
    "        loss_spa = torch.mean(L_spa(F1, F))\n",
    "        loss_col = 5*torch.mean(L_color(F1))\n",
    "        loss_exp = 10*torch.mean(L_exp(F1))\n",
    "        loss =  loss_spa + loss_col+loss_exp\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    step = 0\n",
    "    epoch = epoch+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "723dd818-108a-4ef8-88c8-edf61cb0bd3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([614, 1000, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35290769-b3b7-47bf-827d-8028c3b9353d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([614, 1000, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abca37-47d7-450f-a7de-a9defa7cc407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2c_env",
   "language": "python",
   "name": "c2c_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
